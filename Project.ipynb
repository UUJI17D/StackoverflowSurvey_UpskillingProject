{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The influence of AI on StackOverflow users\n",
    "\n",
    "In the following analysis, I want to take data from the last years of StackOverflow user surveys in order to study how the *growing influence of AI affected the StackOverflow users*. To be more specific, the influence of AI will be assessed by studying\n",
    "1. the development of the **AI-related survey questions**,\n",
    "2. how the users **perceived AI** and if this perception has changed over the years,\n",
    "3. for **which purpose** AI is used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Optional, Dict, List, Tuple, Literal\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survey data is loaded into a dictionary with each dictionary key-value pair indicating a specific survey and the corresponding survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Load the data of several StackOverflow user surveys into data frames\n",
    "# -------------------------------------------------------------------\n",
    "def load_stackoverflow_data(folder_path, load_schema=False):\n",
    "    \"\"\"\n",
    "    Load Stack Overflow survey or schema CSV files from a folder into a dictionary of DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        load_schema (bool): If True, load schema files (filenames containing 'schema').\n",
    "                            If False, load survey response files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: { 'survey_<year>': DataFrame }\n",
    "    \"\"\"\n",
    "    # Determine filter based on schema flag\n",
    "    if load_schema:\n",
    "        csv_files = [f for f in glob.glob(os.path.join(folder_path, \"*.csv\")) if \"schema\" in os.path.basename(f).lower()]\n",
    "    else:\n",
    "        csv_files = [f for f in glob.glob(os.path.join(folder_path, \"*.csv\")) if \"schema\" not in os.path.basename(f).lower()]\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for file in csv_files:\n",
    "        filename = os.path.basename(file)\n",
    "        \n",
    "        # Extract year from filename\n",
    "        match = re.search(r\"(20)\\d{2}\", filename)\n",
    "        year = int(match.group()) if match else None\n",
    "        \n",
    "        # Read CSV with encoding fallback\n",
    "        try:\n",
    "            df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file, encoding=\"latin1\")\n",
    "        \n",
    "        # Add Year column\n",
    "        df[\"Year\"] = year\n",
    "        \n",
    "        # Create dictionary key\n",
    "        key_name = f\"survey_{year}\" if year else \"survey_unknown\"\n",
    "        data_dict[key_name] = df\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs = load_stackoverflow_data(folder_path=\"data\", load_schema=True)\n",
    "survey_dfs = load_stackoverflow_data(folder_path=\"data\", load_schema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that we get the same surveys with the survey data and the additional, explanatory schema data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data assessment and wrangling\n",
    "\n",
    "Let's take a closer look at the survey data and find the questions that are helpful to investigate the influence of AI on the StackOverflow users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Definition of helper functions for data assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Find common columns across a set of dataframes\n",
    "# -------------------------------------------------------------------\n",
    "def find_common_columns(df_set):\n",
    "    \"\"\"\n",
    "    Find columns that appear in at least two DataFrames and show which surveys they belong to.\n",
    "    Sorted by frequency (descending).\n",
    "\n",
    "    Args:\n",
    "        df_set (dict): Dictionary of DataFrames {survey_name: DataFrame}\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: [(column_name, count, [surveys]), ...] sorted by count desc\n",
    "    \"\"\"\n",
    "    # Map each column to the surveys it appears in\n",
    "    column_map = defaultdict(list)\n",
    "    for survey_name, df in df_set.items():\n",
    "        for col in df.columns:\n",
    "            column_map[col].append(survey_name)\n",
    "\n",
    "    # Keep only columns that appear in at least two surveys\n",
    "    common_columns = {col: surveys for col, surveys in column_map.items() if len(surveys) >= 2}\n",
    "\n",
    "    # Sort by frequency\n",
    "    common_sorted = sorted(\n",
    "        [(col, len(surveys), surveys) for col, surveys in common_columns.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(\"Columns common to at least two surveys (sorted by frequency):\")\n",
    "    for col, count, surveys in common_sorted:\n",
    "        print(f\"{col} ({count} surveys): {surveys}\")\n",
    "\n",
    "    return common_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Find columns that are unique to a single survey\n",
    "# -------------------------------------------------------------------\n",
    "def find_unique_columns(df_set):\n",
    "    \"\"\"\n",
    "    Find columns that are unique to a single survey, list them by survey, and provide a summary count.\n",
    "\n",
    "    Args:\n",
    "        df_set (dict): Dictionary of DataFrames {survey_name: DataFrame}\n",
    "\n",
    "    Returns:\n",
    "        dict: {survey: [unique_columns]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Map each column to the surveys it appears in\n",
    "    column_map = defaultdict(list)\n",
    "    for survey_name, df in df_set.items():\n",
    "        for col in df.columns:\n",
    "            column_map[col].append(survey_name)\n",
    "\n",
    "    # Filter unique columns\n",
    "    unique_columns = {col: surveys for col, surveys in column_map.items() if len(surveys) == 1}\n",
    "\n",
    "    # Organize by survey\n",
    "    survey_unique_map = defaultdict(list)\n",
    "    for col, surveys in unique_columns.items():\n",
    "        survey_unique_map[surveys[0]].append(col)\n",
    "\n",
    "    # Sort surveys alphabetically and columns inside each survey\n",
    "    survey_unique_map = {survey: sorted(cols) for survey, cols in sorted(survey_unique_map.items())}\n",
    "\n",
    "    # Print results\n",
    "    print(\"Unique columns per survey:\")\n",
    "    for survey, cols in survey_unique_map.items():\n",
    "        print(f\"{survey} ({len(cols)} unique columns): {cols}\")\n",
    "\n",
    "    return survey_unique_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Find questions across all surveys by keyword\n",
    "# -------------------------------------------------------------------\n",
    "def find_questions_by_keyword(schema_dfs, keyword,\n",
    "                               col_name_cols=(\"Column\", \"qname\"),\n",
    "                               question_cols=(\"question\", \"QuestionText\", \"Question\"),\n",
    "                               case=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Find all questions containing a given keyword AND a question mark across multiple schema DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "        schema_dfs (dict): {survey_name: DataFrame}\n",
    "        keyword (str): keyword to search for\n",
    "        col_name_cols (tuple): possible column name fields\n",
    "        question_cols (tuple): possible question text fields\n",
    "        case (bool): case-sensitive search (default False)\n",
    "        verbose (bool): if True, prints grouped results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: ['survey', 'column_name', 'question']\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    pattern = f\"(?=.*{re.escape(keyword)})(?=.*[?？])\"  # keyword + question mark\n",
    "\n",
    "    for survey_name, schema in schema_dfs.items():\n",
    "        col_name_col = next((c for c in col_name_cols if c in schema.columns), None)\n",
    "        q_col = next((c for c in question_cols if c in schema.columns), None)\n",
    "        if not col_name_col or not q_col:\n",
    "            continue\n",
    "\n",
    "        mask = schema[q_col].astype(str).str.contains(pattern, case=case, na=False, regex=True)\n",
    "        matches = schema.loc[mask, [col_name_col, q_col]].copy()\n",
    "        if not matches.empty:\n",
    "            matches[\"survey\"] = survey_name\n",
    "            matches.rename(columns={col_name_col: \"column_name\", q_col: \"question\"}, inplace=True)\n",
    "            rows.append(matches)\n",
    "\n",
    "    if not rows:\n",
    "        if verbose:\n",
    "            print(f\"No matches found for keyword '{keyword}' with a question mark.\")\n",
    "        return pd.DataFrame(columns=[\"survey\", \"column_name\", \"question\"])\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Found {len(df)} matches for keyword '{keyword}':\")\n",
    "        for survey in df[\"survey\"].unique():\n",
    "            print(f\"\\n--- {survey} ---\")\n",
    "            subset = df[df[\"survey\"] == survey]\n",
    "            for _, row in subset.iterrows():\n",
    "                print(f\"{row['column_name']}: {row['question']}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Find either duplicated questions or column names across all surveys\n",
    "# -------------------------------------------------------------------\n",
    "def find_duplicates_across_surveys(df, check_on=\"question\", duplicate_min=2, only_duplicates=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Find duplicates across surveys based on question text OR column_name.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Output from find_questions_by_keyword or similar\n",
    "        check_on (str): 'question' or 'column_name' (field to check duplicates on)\n",
    "        duplicate_min (int): Minimum number of surveys for a duplicate (default 2)\n",
    "        only_duplicates (bool): If True, return only rows that meet the duplicate_min threshold\n",
    "        verbose (bool): if True, prints grouped results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: ['survey', 'column_name', 'question', 'survey_count', 'duplicate_flag']\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import unicodedata\n",
    "\n",
    "    if df.empty:\n",
    "        if verbose:\n",
    "            print(\"No data to check for duplicates.\")\n",
    "        return pd.DataFrame(columns=[\"survey\", \"column_name\", \"question\", \"survey_count\", \"duplicate_flag\"])\n",
    "\n",
    "    # Normalization for grouping\n",
    "    def normalize(text):\n",
    "        text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "        text = text.replace(\"\\u00A0\", \" \").replace(\"\\u202F\", \" \")\n",
    "        text = re.sub(r\"[\\u200B-\\u200D\\u2060\\uFEFF]\", \"\", text)\n",
    "        text = text.replace(\"*\", \"\").replace(\"＊\", \"\")\n",
    "        text = re.sub(r\"\\s+\", \" \", text.lower()).strip()\n",
    "        return text\n",
    "\n",
    "    if check_on not in [\"question\", \"column_name\"]:\n",
    "        raise ValueError(\"check_on must be 'question' or 'column_name'\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"normalized\"] = df[check_on].apply(normalize)\n",
    "    df = df[df[\"normalized\"].ne(\"\")]\n",
    "\n",
    "    # Count surveys per normalized value\n",
    "    survey_counts = df.groupby(\"normalized\")[\"survey\"].nunique()\n",
    "    df[\"survey_count\"] = df[\"normalized\"].map(survey_counts)\n",
    "    df[\"duplicate_flag\"] = df[\"survey_count\"] >= duplicate_min\n",
    "\n",
    "    # Filter if only duplicates requested\n",
    "    if only_duplicates:\n",
    "        df = df[df[\"duplicate_flag\"]]\n",
    "\n",
    "    df = df.drop(columns=\"normalized\").sort_values(\n",
    "        by=[\"duplicate_flag\", \"survey_count\", \"survey\"], ascending=[False, False, True]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    if verbose and not df.empty:\n",
    "        print(f\"Duplicates based on {check_on} (min surveys = {duplicate_min}):\")\n",
    "        for survey in df[\"survey\"].unique():\n",
    "            print(f\"\\n--- {survey} ---\")\n",
    "            subset = df[df[\"survey\"] == survey]\n",
    "            for _, row in subset.iterrows():\n",
    "                marker = f\" (DUPLICATE, {row['survey_count']})\" if row['survey_count'] >= duplicate_min else \"\"\n",
    "                print(f\"{row['column_name']}: {row['question']}{marker}\")\n",
    "    elif verbose:\n",
    "        print(\"No duplicates found.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Retrieve the question for a given column name across all surveys\n",
    "# -------------------------------------------------------------------\n",
    "def get_question_description(column_name, schema_dfs,\n",
    "                    col_name_cols=(\"Column\", \"qname\"),\n",
    "                    question_cols=(\"question\", \"QuestionText\", \"Question\"),\n",
    "                    return_all=True):\n",
    "    \"\"\"\n",
    "    Retrieve the description of a column from multiple schema DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "        column_name (str): The name of the column you want to look up.\n",
    "        schema_dfs (dict): {survey_name: DataFrame} of schema tables.\n",
    "        col_name_cols (tuple): Possible column names that identify the column name in the schema.\n",
    "        question_cols (tuple): Possible column names that contain the question text.\n",
    "        return_all (bool): If True, return all matches as a dict {survey: description}. \n",
    "                           If False, return the first match.\n",
    "\n",
    "    Returns:\n",
    "        str or dict: The description of the column (first match) or a dict of all matches.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for survey_name, schema in schema_dfs.items():\n",
    "        # Find which column in schema holds the column names\n",
    "        col_name_col = next((c for c in col_name_cols if c in schema.columns), None)\n",
    "        if not col_name_col:\n",
    "            continue\n",
    "\n",
    "        # Find which column in schema holds the question text\n",
    "        q_col = next((c for c in question_cols if c in schema.columns), None)\n",
    "        if not q_col:\n",
    "            continue\n",
    "\n",
    "        # Filter for the requested column_name\n",
    "        match = schema.loc[schema[col_name_col] == column_name, q_col]\n",
    "\n",
    "        if not match.empty:\n",
    "            results[survey_name] = match.iloc[0]\n",
    "            if not return_all:\n",
    "                return match.iloc[0]\n",
    "\n",
    "    return results if results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Retrieve the column name for a given question across all surveys\n",
    "# -------------------------------------------------------------------\n",
    "def get_column_name(question_text, schema_dfs,\n",
    "                    col_name_cols=(\"Column\", \"qname\"),\n",
    "                    question_cols=(\"question\", \"QuestionText\", \"Question\"),\n",
    "                    case=False, return_all=True):\n",
    "    \"\"\"\n",
    "    Find column name(s) for a given question text from multiple schema DataFrames.\n",
    "    Handles case-insensitive and partial matches, and escapes regex characters.\n",
    "\n",
    "    Parameters:\n",
    "        question_text (str): The question text (or part of it) to search for.\n",
    "        schema_dfs (dict): {survey_name: DataFrame} of schema tables.\n",
    "        col_name_cols (tuple): Possible column name fields in schema.\n",
    "        question_cols (tuple): Possible question text fields in schema.\n",
    "        case (bool): Case-sensitive search (default False).\n",
    "        return_all (bool): If True, return all matches as {survey: [columns]}.\n",
    "                           If False, return the first match found.\n",
    "\n",
    "    Returns:\n",
    "        dict or str or None\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Normalize query: collapse whitespace and lowercase if case=False\n",
    "    query = \" \".join(question_text.split())\n",
    "    if not case:\n",
    "        query = query.lower()\n",
    "\n",
    "    for survey_name, schema in schema_dfs.items():\n",
    "        # Detect relevant columns\n",
    "        col_name_col = next((c for c in col_name_cols if c in schema.columns), None)\n",
    "        q_col = next((c for c in question_cols if c in schema.columns), None)\n",
    "        if not col_name_col or not q_col:\n",
    "            continue\n",
    "\n",
    "        # Normalize schema question text\n",
    "        q_series = schema[q_col].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "        if not case:\n",
    "            q_series = q_series.str.lower()\n",
    "\n",
    "        # Escape regex special chars in query for literal match\n",
    "        pattern = re.escape(query)\n",
    "\n",
    "        mask = q_series.str.contains(pattern, na=False)\n",
    "        matches = schema.loc[mask, col_name_col]\n",
    "\n",
    "        if not matches.empty:\n",
    "            results[survey_name] = matches.tolist()\n",
    "            if not return_all:\n",
    "                return matches.iloc[0]\n",
    "\n",
    "    return results if results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Calculate the value count of a given column across all surveys\n",
    "# -------------------------------------------------------------------\n",
    "def value_counts_all_surveys(survey_dfs, column_name, normalize=False, dropna=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute value counts for a given column across all surveys, with optional verbose output.\n",
    "\n",
    "    Parameters:\n",
    "        survey_dfs (dict): {survey_name: DataFrame}\n",
    "        column_name (str): Column to compute value counts for\n",
    "        normalize (bool): If True, return relative frequencies (like pandas normalize)\n",
    "        dropna (bool): If True, exclude NaN values\n",
    "        verbose (bool): If True, print detailed info for each survey\n",
    "\n",
    "    Returns:\n",
    "        dict: {survey_name: Series of value counts}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for survey_name, df in survey_dfs.items():\n",
    "        if column_name in df.columns:\n",
    "            counts = df[column_name].value_counts(normalize=normalize, dropna=dropna)\n",
    "            results[survey_name] = counts\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n--- Survey: {survey_name} ---\")\n",
    "                print(f\"Column analyzed: {column_name}\")\n",
    "                print(f\"Total rows: {len(df)}\")\n",
    "                print(f\"Unique values: {df[column_name].nunique(dropna=dropna)}\")\n",
    "                print(f\"NaN values: {df[column_name].isna().sum()}\")\n",
    "                print(\"Value counts:\")\n",
    "                print(counts.to_string())  # Full output without truncation\n",
    "        else:\n",
    "            results[survey_name] = None\n",
    "            if verbose:\n",
    "                print(f\"\\n--- Survey: {survey_name} ---\")\n",
    "                print(f\"Column '{column_name}' not found in this survey.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Replace missing values in the survey data with \"no answer\"\n",
    "# -------------------------------------------------------------------\n",
    "def replace_missing_all_columns(\n",
    "    survey_dfs: Dict[str, pd.DataFrame],\n",
    "    na_label: str = \"No answer\",\n",
    "    treat_blank_as_na: bool = True,\n",
    "    as_category: bool = True,\n",
    "    inplace: bool = False\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Replace NaN (and optionally blank strings) in ALL columns of each survey DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    survey_dfs : dict[str, pd.DataFrame]\n",
    "        Mapping of survey name -> DataFrame.\n",
    "    na_label : str, default \"No answer\"\n",
    "        Label to use for missing values.\n",
    "    treat_blank_as_na : bool, default True\n",
    "        If True, empty/whitespace-only strings are treated as NaN before replacement.\n",
    "    as_category : bool, default True\n",
    "        If True, convert all object/string columns to 'category' after replacement.\n",
    "    inplace : bool, default False\n",
    "        If True, modify DataFrames in `survey_dfs` directly and return the same dict.\n",
    "        If False, return a new dict with copies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, pd.DataFrame]\n",
    "        Dict of processed DataFrames (same object if inplace=True).\n",
    "    \"\"\"\n",
    "    out = survey_dfs if inplace else {}\n",
    "\n",
    "    for name, df in survey_dfs.items():\n",
    "        target = df if inplace else df.copy()\n",
    "\n",
    "        # Optionally treat blanks as NaN\n",
    "        if treat_blank_as_na:\n",
    "            target = target.replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "\n",
    "        # Replace all NaNs with na_label\n",
    "        target = target.fillna(na_label)\n",
    "\n",
    "        # Convert object columns to category if requested\n",
    "        if as_category:\n",
    "            for col in target.columns:\n",
    "                if target[col].dtype == object or pd.api.types.is_string_dtype(target[col]):\n",
    "                    target[col] = target[col].astype(\"category\")\n",
    "\n",
    "        if not inplace:\n",
    "            out[name] = target\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 General data assessment\n",
    "\n",
    "First, let's make a copy of the original dataframe in case we need to make changes for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs_original = schema_dfs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_common_cols = find_common_columns(survey_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite many columns that appear in several surveys. However, the most common ones do not seem to be obviously linked to AI so we have to investigate further and maybe restrict ourselves to the surveys of the last couple of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_unique_cols = find_unique_columns(survey_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can already see more AI-related questions. As they are all unique, they are difficult to compare over a timescale. However, this shows that questions related to AI have changed a lot which is in line with the dynamic development of AI itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_common_cols = find_common_columns(schema_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the schema data has changed over the years, which will make it increasingly difficult for an analysis. However, if we restrict ourselves to the last few years the schema data should be roughly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_unique_cols = find_unique_columns(schema_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Question 1: Assessment of AI-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = find_questions_by_keyword(schema_dfs, \"AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like on the one hand the number of AI-related questions has increased a lot in the last years. On the other hand, we still need to find questions that we can use to compare the development over several years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches[\"survey\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_counts_AI = df_matches[\"survey\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years from the survey column (assuming format like 'survey_2018')\n",
    "df_matches['year'] = df_matches['survey'].str.extract(r'(\\d{4})')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(\n",
    "    data=df_matches,\n",
    "    x='year',\n",
    "    order=df_matches['survey'].str.extract(r'(\\d{4})')[0].drop_duplicates(),\n",
    "    color='royalblue'\n",
    ")\n",
    "\n",
    "# Annotate each bar with integer count\n",
    "for p in ax.patches:\n",
    "    height = int(p.get_height())\n",
    "    ax.text(\n",
    "        x=p.get_x() + p.get_width() / 2,\n",
    "        y=height + 0.5,\n",
    "        s=f\"N={height}\",\n",
    "        ha=\"center\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Number of survey questions related to AI\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Survey\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the number of AI-related questions has increased significantly with 31 questions now in 2025. How is this comparable to the total number of questions of the surveys though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of columns in each survey\n",
    "survey_column_counts = {key: df.shape[1] for key, df in survey_dfs.items()}\n",
    "\n",
    "# Count AI-related questions in df_matches for each survey\n",
    "ai_question_counts = df_matches[\"survey\"].value_counts().to_dict()\n",
    "\n",
    "# Prepare data\n",
    "years = sorted([key.split('_')[1] for key in survey_column_counts.keys()])\n",
    "total_counts = [survey_column_counts[f\"survey_{year}\"] for year in years]\n",
    "ai_counts = [ai_question_counts.get(f\"survey_{year}\", 0) for year in years]\n",
    "non_ai_counts = [total - ai for total, ai in zip(total_counts, ai_counts)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(years, non_ai_counts, label=\"Non-AI Questions\", color=\"lightgray\")\n",
    "plt.bar(years, ai_counts, bottom=non_ai_counts, label=\"AI Questions\", color=\"royalblue\")\n",
    "\n",
    "# Annotate both total and AI counts\n",
    "for i, (total, ai) in enumerate(zip(total_counts, ai_counts)):\n",
    "    plt.text(i, total + 1, f\"AI={ai}\", ha='center', fontsize=9)\n",
    "    \n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Questions\")\n",
    "plt.title(\"Total and AI-Related Questions per Survey Year\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this plot, we can see that there was a drastic decrease in the overall number of questions around 2020/2021 which might be related to the pandemic. Since 2023, the number of questions rises again in general as well as the proportion of AI-related questions, which now in 2025 makes up almost 20% of the whole survey. This is already a very good indication of the **growing influence of AI among the stackoverflow users**, which closes our first field of interest to look into the development of the AI-related questions over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data assessment for question 2: How do StackOverflow users perceive data?\n",
    "\n",
    "Next, we want to see which questions we can use to increase our understanding of **how AI is perceived** by the stackoverlow users and how this perception might have changed over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups_question = find_duplicates_across_surveys(df_matches, check_on=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups_question = find_duplicates_across_surveys(df_matches, check_on=\"question\", duplicate_min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be only three questions that are identical across at least three surveys, which can be considered as the minimum for a meaningful time analysis. But let's look also at the question identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups_question = find_duplicates_across_surveys(df_matches, check_on=\"column_name\", duplicate_min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when also taking into account the column identifier names, there is one more question \"AITool\" we can use for our analysis. This was not found before, because the actual wording has changed from 2024 to 2025. To be more precise, this question entails more subquestions in 2025 than before. This will be a very useful question in order to tackle question 3 later.\n",
    "\n",
    "Let's start with the three questions \"AISelect\", \"AISent\", and \"AIAcc\" in order to study the perception of AI among the StackOverflow users. First, let's assess the data of these questions so we can see if we need to perform any data wrangling for the purpose of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_description(\"AIBen\", schema_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_description(\"AIAcc\", schema_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is an error in the survey of 2023 mixing up the questions \"AIBen\" related to benefits and \"AIAcc\" for accuracy. In order to properly compare the answers over the surveys of 2023, 2024, and 2025 we need to adjust the naming of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs[\"survey_2023\"][schema_dfs[\"survey_2023\"][\"qname\"]==\"AIBen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_dfs[\"survey_2023\"][\"AIBen\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_name(\"For the AI tools you use as part of your development workflow, what are the MOST important benefits you are hoping to achieve? Please check all that apply.\", schema_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_name(\"How much do you trust the accuracy of the output from AI tools as part of your development workflow?\", schema_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assign the correct column name to the questions \"AIBen\" and \"AIAcc\" in the survey 2023, we need to replace it both in the survey and in the schema data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs[\"survey_2023\"][\"qname\"] = schema_dfs[\"survey_2023\"][\"qname\"].replace({\"AIBen\": \"AIAcc\", \"AIAcc\": \"AIBen\"})\n",
    "survey_dfs[\"survey_2023\"].rename(columns={\"AIBen\": \"AIAcc\", \"AIAcc\": \"AIBen\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_description(\"AIBen\", schema_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_description(\"AIAcc\", schema_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = find_questions_by_keyword(schema_dfs, \"AI\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups_question = find_duplicates_across_surveys(df_matches, check_on=\"question\", duplicate_min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dups_question = find_duplicates_across_surveys(df_matches, check_on=\"column_name\", duplicate_min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the replacement worked. Let's check the variety of answeres for the questions \"AISelect\", \"AISent\", and \"AIAcc\" next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_AISelect = value_counts_all_surveys(survey_dfs, 'AISelect', normalize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_AIAcc = value_counts_all_surveys(survey_dfs, 'AIAcc', normalize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_AISent = value_counts_all_surveys(survey_dfs, 'AISent', normalize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address the NaNs in the surveys by replacing them with \"no answer\" in order to also see how many survey participants did not answer a specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_dfs = replace_missing_all_columns(\n",
    "    survey_dfs,\n",
    "    na_label=\"No answer\",\n",
    "    treat_blank_as_na=True,\n",
    "    as_category=True,\n",
    "    inplace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_AISent = value_counts_all_surveys(survey_dfs, 'AISent', normalize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, now this data is ready to be analyzed in order to answer question 2 about how AI is perceived among the StackOverflow users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Data assessment for question 3: for which purpose is AI used for?\n",
    "\n",
    "Next, we want to investigate the column \"AITool\" in order to be able to study for what kind of tools or work the survey participants are using AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_AITool = value_counts_all_surveys(survey_dfs, 'AITool', normalize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although \"AITool\" is a valid column name and found in the schema of 2023, 2024, and 2025 it cannot be found in the actual survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_description(\"AITool\", schema_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in schema_dfs[\"survey_2023\"][\"qname\"] if \"AITool\" in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs[\"survey_2023\"][schema_dfs[\"survey_2023\"][\"qname\"]==\"AITool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in schema_dfs[\"survey_2024\"][\"qname\"] if \"AITool\" in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs[\"survey_2024\"][schema_dfs[\"survey_2024\"][\"qname\"]==\"AITool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in schema_dfs[\"survey_2025\"][\"qname\"] if \"AITool\" in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dfs[\"survey_2025\"][schema_dfs[\"survey_2025\"][\"qname\"]==\"AITool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the \"qname\" parameter is identical over the surveys and the questions only differ slightly, a column called \"AITool\" cannot be found in the survey data. So let's see if we can find a close if not identifal match among the survey column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_col_names = [col for col in survey_dfs[\"survey_2023\"].columns if \"AITool\" in col]\n",
    "survey_dfs[\"survey_2023\"][tool_col_names].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the subquestions for \"AITool\" are already in separate columns in the survey data of 2023. We need to check if this is also the case for the surveys of 2023 and 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_col_names = [col for col in survey_dfs[\"survey_2024\"].columns if \"AITool\" in col]\n",
    "survey_dfs[\"survey_2024\"][tool_col_names].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survey of 2024 uses the exact same column names for the subquestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_col_names = [col for col in survey_dfs[\"survey_2025\"].columns if \"AITool\" in col]\n",
    "survey_dfs[\"survey_2025\"][tool_col_names].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already stated before, the content of the question changed for the survey of 2025 with several more subquestions and aspects. This can now be seen as we have even more columns for \"AITool\" than for the surveys of 2023 and 2024. In order to be able to compare the answers to these questions at least on the level that is provided by the questions of 2023 and 2024, we need to rewrangling the columns in the 2025 survey so the columns for \"AITool\" match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_dfs[\"survey_2025\"][\"AIToolCurrently Using\"] = survey_dfs[\"survey_2025\"][['AIToolCurrently partially AI', 'AIToolCurrently mostly AI']].apply(lambda x: ';'.join([str(val) for val in x if pd.notna(val) and val != '']), axis=1)\n",
    "survey_dfs[\"survey_2025\"][\"AIToolInterested in Using\"] = survey_dfs[\"survey_2025\"][['AIToolPlan to partially use AI', 'AIToolPlan to mostly use AI']].apply(lambda x: ';'.join([str(val) for val in x if pd.notna(val) and val != '']), axis=1)\n",
    "survey_dfs[\"survey_2025\"].rename(columns={\"AIToolDon't plan to use AI for this task\": \"AIToolNot interested in Using\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "survey_dfs[\"survey_2025\"].drop(columns=[\n",
    "    'AIToolCurrently partially AI',\n",
    "    'AIToolCurrently mostly AI',\n",
    "    'AIToolPlan to partially use AI',\n",
    "    'AIToolPlan to mostly use AI'\n",
    "], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_dfs[\"survey_2025\"][\"AIToolInterested in Using\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_col_names_2 = [col for col in survey_dfs[\"survey_2025\"].columns if \"AITool\" in col]\n",
    "survey_dfs[\"survey_2025\"][tool_col_names_2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now also the data for \"AITool\" looks ready to be analyzed in order to answer the question 3 related to the purpose of the AI usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis of the StackOverflow user answers\n",
    "\n",
    "### 3.1 Definition of functions to analyze and plot the relevant data\n",
    "\n",
    "In order to analyze and plot the answers of the StackOverflow users for our four chosen questions \"AISelect\", \"AISent\", \"AIBen\", and \"AITool\" it will be helpful to define some functions that we can then call for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Combine surveys into one DataFrame with a Year column\n",
    "# -------------------------------------------------------------------\n",
    "def prepare_long_format(survey_dict: Dict[str, pd.DataFrame], columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine multiple yearly survey DataFrames into a single long-format DataFrame\n",
    "    that includes a 'Year' column. Only the specified columns + 'Year' are kept.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    survey_dict : dict[str, pd.DataFrame]\n",
    "        Mapping of year label to DataFrame, e.g. {\"2023\": df2023, \"2024\": df2024, ...}\n",
    "    columns : list[str]\n",
    "        Columns to carry forward. These are expected to contain semicolon-separated strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Concatenated DataFrame with given columns and a numeric 'Year' column.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for year, df in survey_dict.items():\n",
    "        temp = df.copy()\n",
    "        temp[\"Year\"] = int(year)\n",
    "        keep_cols = [c for c in columns if c in temp.columns]\n",
    "        frames.append(temp[keep_cols + [\"Year\"]])\n",
    "    return pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Build Answer × Year × Category counts (generic categories)\n",
    "# -------------------------------------------------------------------\n",
    "def compute_trend_generic(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    category_map: Optional[Dict[str, str]] = None,\n",
    "    answers: Optional[List[str]] = None,\n",
    "    deduplicate_within_row: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Parse semicolon-separated multi-select columns to produce a long table of\n",
    "    Answer × Year × Category counts. Categories are derived from column names\n",
    "    or from an optional mapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input frame from `prepare_long_format`, must include a 'Year' column and the target columns.\n",
    "    columns : list[str]\n",
    "        Columns to analyze (each is a semicolon-separated multi-select).\n",
    "    category_map : dict[str, str], optional\n",
    "        Mapping from column name -> display category. If None, uses column names as-is.\n",
    "    answers : list[str], optional\n",
    "        If provided, restrict counts to this set of answers.\n",
    "    deduplicate_within_row : bool\n",
    "        If True, within a single respondent row, count each (answer, category) at most once.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trend_df : pd.DataFrame\n",
    "        Long table with columns: ['Answer', 'Year', 'Category', 'Count'].\n",
    "    base_counts : pd.DataFrame\n",
    "        Table with columns: ['Year', 'N_respondents'] (total rows per year), used for relative='year'.\n",
    "    categories : list[str]\n",
    "        List of category names used (order preserved from mapping or columns).\n",
    "    \"\"\"\n",
    "    if category_map is None:\n",
    "        category_map = {col: col for col in columns}\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        year = row[\"Year\"]\n",
    "        seen = set()\n",
    "        for col in columns:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            if pd.isna(row[col]):\n",
    "                continue\n",
    "            category = category_map.get(col, col)\n",
    "            # Split semicolon-separated values\n",
    "            for ans in (a.strip() for a in str(row[col]).split(';')):\n",
    "                if not ans:\n",
    "                    continue\n",
    "                if answers is not None and ans not in answers:\n",
    "                    continue\n",
    "                if deduplicate_within_row:\n",
    "                    key = (ans, category)\n",
    "                    if key in seen:\n",
    "                        continue\n",
    "                    seen.add(key)\n",
    "                records.append((ans, year, category))\n",
    "\n",
    "    trend_df = (\n",
    "        pd.DataFrame(records, columns=[\"Answer\", \"Year\", \"Category\"])\n",
    "          .groupby([\"Answer\", \"Year\", \"Category\"], as_index=False)\n",
    "          .size()\n",
    "          .rename(columns={\"size\": \"Count\"})\n",
    "    )\n",
    "\n",
    "    base_counts = (\n",
    "        df.groupby(\"Year\", as_index=False)\n",
    "          .size()\n",
    "          .rename(columns={\"size\": \"N_respondents\"})\n",
    "    )\n",
    "\n",
    "    categories = list(dict.fromkeys(category_map.values()))  # preserve insertion order\n",
    "    return trend_df, base_counts, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Add relative modes for plotting\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "Mode = Literal[\"year\", \"answer\"]\n",
    "\n",
    "def add_relative(\n",
    "    trend_df: pd.DataFrame,\n",
    "    base_counts: pd.DataFrame,\n",
    "    mode: Optional[Mode] = None,\n",
    ") -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Add a 'Value' column for plotting based on the chosen relative mode.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trend_df : pd.DataFrame\n",
    "        Must have columns ['Answer', 'Year', 'Category', 'Count'].\n",
    "    base_counts : pd.DataFrame\n",
    "        Must have columns ['Year', 'N_respondents'] (required only if mode == \"year\").\n",
    "    mode : {\"year\", \"answer\"} or None, optional\n",
    "        - None: use raw counts (Value = Count).\n",
    "        - \"year\": Value = Count / N_respondents(year) * 100 (percent of all respondents that year).\n",
    "        - \"answer\": Value = Count / sum(Count by Answer & Year) * 100 (category share within answer-year).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_with_value : pd.DataFrame\n",
    "        Copy of trend_df with an added 'Value' column.\n",
    "    y_label : str\n",
    "        Recommended y-axis label for the plot.\n",
    "    \"\"\"\n",
    "    # --- Validate inputs early (clear error messages help downstream users) ---\n",
    "    required_trend = {\"Answer\", \"Year\", \"Category\", \"Count\"}\n",
    "    missing_trend = required_trend - set(trend_df.columns)\n",
    "    if missing_trend:\n",
    "        raise ValueError(f\"trend_df is missing required columns: {sorted(missing_trend)}\")\n",
    "\n",
    "    if mode == \"year\":\n",
    "        required_base = {\"Year\", \"N_respondents\"}\n",
    "        missing_base = required_base - set(base_counts.columns)\n",
    "        if missing_base:\n",
    "            raise ValueError(f\"base_counts is missing required columns: {sorted(missing_base)}\")\n",
    "\n",
    "    df = trend_df.copy()\n",
    "\n",
    "    # --- Raw counts ---\n",
    "    if mode is None:\n",
    "        df[\"Value\"] = df[\"Count\"]\n",
    "        return df, \"Count\"\n",
    "\n",
    "    # --- Percent of all respondents in that year ---\n",
    "    if mode == \"year\":\n",
    "        # m:1 validates that each Year maps to at most one N_respondents row\n",
    "        df = df.merge(\n",
    "            base_counts[[\"Year\", \"N_respondents\"]],\n",
    "            on=\"Year\",\n",
    "            how=\"left\",\n",
    "            validate=\"m:1\",\n",
    "        )\n",
    "        if df[\"N_respondents\"].isna().any():\n",
    "            raise ValueError(\"Missing N_respondents for some Year values after merge.\")\n",
    "\n",
    "        denom = df[\"N_respondents\"].replace(0, np.nan)\n",
    "        df[\"Value\"] = df[\"Count\"].div(denom).mul(100).fillna(0.0)\n",
    "        return df, \"Proportion of all respondents (%)\"\n",
    "\n",
    "    # --- Share within (Answer, Year) ---\n",
    "    if mode == \"answer\":\n",
    "        # Named aggregation avoids a post-rename step (and the typing complaint)\n",
    "        totals = (\n",
    "            df.groupby([\"Answer\", \"Year\"], as_index=False)\n",
    "              .agg(AnswerYearTotal=(\"Count\", \"sum\"))\n",
    "        )\n",
    "        df = df.merge(totals, on=[\"Answer\", \"Year\"], how=\"left\", validate=\"m:1\")\n",
    "        denom = df[\"AnswerYearTotal\"].replace(0, np.nan)\n",
    "        df[\"Value\"] = df[\"Count\"].div(denom).mul(100).fillna(0.0)\n",
    "        return df, \"Category share within answer-year (%)\"\n",
    "\n",
    "    raise ValueError(f\"Unknown mode: {mode!r}. Use None, 'year', or 'answer'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Answer selection (global top-N or union of per-year top-N)\n",
    "# -------------------------------------------------------------------\n",
    "def select_answers(\n",
    "    trend_df: pd.DataFrame,\n",
    "    top_n: Optional[int] = None,\n",
    "    ensure_top_n_per_year: bool = False,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Select which answers to include in plots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trend_df : pd.DataFrame\n",
    "        Must have columns ['Answer', 'Year', 'Category', 'Count'].\n",
    "    top_n : int, optional\n",
    "        If None, returns all answers. If provided, returns a subset.\n",
    "    ensure_top_n_per_year : bool\n",
    "        - If True: returns the union of per-year top_n answers (by total across categories).\n",
    "        - If False: returns the global top_n answers by total across all years & categories.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Selected answers to plot (deduplicated).\n",
    "    \"\"\"\n",
    "    # --- Validate required columns up front ---\n",
    "    required = {\"Answer\", \"Year\", \"Category\", \"Count\"}\n",
    "    missing = required - set(trend_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"trend_df is missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # --- All answers (original order preserved) ---\n",
    "    if top_n is None:\n",
    "        # drop_duplicates keeps first occurrence order; cast to str for type safety\n",
    "        return trend_df[\"Answer\"].astype(str).drop_duplicates().tolist()\n",
    "\n",
    "    # --- Degenerate cases ---\n",
    "    if not isinstance(top_n, int):\n",
    "        raise TypeError(f\"top_n must be an int or None, got {type(top_n).__name__}\")\n",
    "    if top_n <= 0:\n",
    "        return []\n",
    "\n",
    "    # --- Per-year top-N union ---\n",
    "    if ensure_top_n_per_year:\n",
    "        per_year = (\n",
    "            trend_df.groupby([\"Year\", \"Answer\"], as_index=False)\n",
    "                    .agg(total=(\"Count\", \"sum\"))\n",
    "        )\n",
    "\n",
    "        # Sort with deterministic tie-break on Answer\n",
    "        per_year_sorted = per_year.sort_values(\n",
    "            by=[\"Year\", \"total\", \"Answer\"],\n",
    "            ascending=[True, False, True],\n",
    "            kind=\"mergesort\",  # stable\n",
    "        )\n",
    "\n",
    "        # Take top_n within each year, then union across years\n",
    "        top_union = (\n",
    "            per_year_sorted.groupby(\"Year\", group_keys=False)\n",
    "                           .head(top_n)[\"Answer\"]\n",
    "                           .astype(str)\n",
    "                           .unique()\n",
    "                           .tolist()\n",
    "        )\n",
    "        return top_union\n",
    "\n",
    "    # --- Global top-N across all years/categories ---\n",
    "    global_totals = (\n",
    "        trend_df.groupby(\"Answer\", as_index=False)\n",
    "                .agg(total=(\"Count\", \"sum\"))\n",
    "                .sort_values(\n",
    "                    by=[\"total\", \"Answer\"],\n",
    "                    ascending=[False, True],\n",
    "                    kind=\"mergesort\",\n",
    "                )\n",
    "    )\n",
    "\n",
    "    return global_totals.head(top_n)[\"Answer\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Plot everything as a heatmap and several bar plots\n",
    "# -------------------------------------------------------------------\n",
    "def plot_combined_dashboard(\n",
    "    trend_df: pd.DataFrame,\n",
    "    base_counts: pd.DataFrame,\n",
    "    categories: List[str],\n",
    "    top_n: int = 8,\n",
    "    ensure_top_n_per_year: bool = False,\n",
    "    strict_top_n: bool = True,\n",
    "    relative_heatmap: Optional[str] = \"year\",   # None -> absolute counts\n",
    "    relative_bars: Optional[str] = \"answer\",    # None -> absolute counts\n",
    "    cmap: str = \"YlGnBu\",\n",
    "    annotate: bool = True,\n",
    "\n",
    "    # Comparability & clarity\n",
    "    share_y: bool = True,\n",
    "    legend_per_subplot: bool = False,\n",
    "    annotate_bar_totals: bool = True,\n",
    "\n",
    "    # Layout\n",
    "    bar_cols: int = 6,\n",
    "    heatmap_width_per_cat: float = 4.5,\n",
    "    bar_width_per_col: float = 4.2,\n",
    "    top_row_height: float = 5.0,\n",
    "    bar_row_height: float = 2.6,\n",
    "\n",
    "    # Readability\n",
    "    font_scale: float = 0.95,\n",
    "    heatmap_tick_fontsize: int = 10,\n",
    "    bar_tick_fontsize: int = 9,\n",
    "    bar_title_fontsize: int = 10,\n",
    "    rotate_bar_xticks: int = 30,\n",
    "\n",
    "    # Spacing\n",
    "    gridspec_hspace: float = 0.45,\n",
    "    heatmap_wspace: float = 0.30,\n",
    "    bar_wspace: float = 0.50,\n",
    "    bar_hspace: float = 0.75,\n",
    "    legend_right_pad: float = 0.18,\n",
    "    cbar_fraction: float = 0.025,\n",
    "    cbar_pad: float = 0.02,\n",
    "\n",
    "    # Labels/annotation\n",
    "    xlabel_pad: float = 4,\n",
    "    ylabel_pad: float = 4,\n",
    "    annot_fontsize: int = 9,\n",
    "    show_all_heatmap_y: bool = False,\n",
    "\n",
    "    # Legend wrapping (Zeilenumbruch)\n",
    "    legend_title: str = \"Category\",\n",
    "    legend_wrap_chars: Optional[int] = 25,\n",
    "    legend_title_wrap_chars: Optional[int] = None,\n",
    "\n",
    "    # Controls y-axis label and y-ticks visibility on bar subplots\n",
    "    show_bar_ylabel: bool = True,  # True: all subplots show y-label & y-ticks; False: only first column\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a combined dashboard with:\n",
    "      • Top row: heatmaps (one per category, shared colorbar).\n",
    "      • Bottom: stacked bar charts (one subplot per answer).\n",
    "      • Optional Totals bar chart: when `relative_bars=None`, a totals subplot is added as the **first** bar.\n",
    "\n",
    "    Key features\n",
    "    ------------\n",
    "    - **Totals bar chart (first position)**: Appears only when `relative_bars=None`, summing across the\n",
    "      selected answers and the displayed categories per year.\n",
    "    - **Legend wrapping (Zeilenumbruch)**: Long legend labels and the legend title break into multiple lines.\n",
    "      Controlled by `legend_wrap_chars` and `legend_title_wrap_chars`.\n",
    "    - **Dynamic layout**: Grid adjusts automatically to the number of answers and `bar_cols`.\n",
    "    - **Comparable scales**: `share_y=True` shares the y-scale across answer subplots (Totals uses its own).\n",
    "    - **Y-axis visibility control**: `show_bar_ylabel` controls both the y-axis label and y-ticks:\n",
    "        * If True: every bar subplot (including Totals) shows y-label and y-ticks.\n",
    "        * If False: only the **first column** of bar subplots (including Totals if placed in col 0) shows y-label and y-ticks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trend_df : pd.DataFrame\n",
    "        Long-format data with columns ['Answer', 'Year', 'Category', 'Count'].\n",
    "    base_counts : pd.DataFrame\n",
    "        Data with ['Year', 'N_respondents'] for normalization when any `relative_* == \"year\"`.\n",
    "    categories : List[str]\n",
    "        Ordered list of category labels for heatmaps and stacked bars.\n",
    "    top_n : int\n",
    "        Number of answers to include.\n",
    "    ensure_top_n_per_year : bool\n",
    "        Include per-year top answers in the union selection.\n",
    "    strict_top_n : bool\n",
    "        Reduce the union back to exactly `top_n` using global totals.\n",
    "    relative_heatmap : {\"year\", None}\n",
    "        Heatmap normalization: None = absolute counts; \"year\" = % of respondents (requires `base_counts`).\n",
    "    relative_bars : {\"answer\", \"year\", None}\n",
    "        Bars normalization: None = absolute counts; \"answer\" = 100% within answer-year; \"year\" = % of respondents.\n",
    "    cmap : str\n",
    "        Colormap for heatmaps.\n",
    "    annotate : bool\n",
    "        Annotate heatmap cells with values.\n",
    "    share_y : bool\n",
    "        Share y-axis across **answer** bar subplots (Totals not shared).\n",
    "    legend_per_subplot : bool\n",
    "        If True, add legend in each bar subplot; otherwise a global legend.\n",
    "    annotate_bar_totals : bool\n",
    "        Annotate raw N above each stacked bar.\n",
    "    show_bar_ylabel : bool\n",
    "        Control y-axis label and y-ticks on bar subplots:\n",
    "         - True: show on **all** bar subplots (including Totals).\n",
    "         - False: show only on the **first column** of the bar grid; hide on other columns.\n",
    "    legend_wrap_chars : int or None\n",
    "        Wrap legend labels at this character width; None disables wrapping.\n",
    "    legend_title_wrap_chars : int or None\n",
    "        Wrap legend title; if None, uses `legend_wrap_chars`.\n",
    "    \"\"\"\n",
    "\n",
    "    import math, textwrap\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.patches import Patch\n",
    "    from typing import Optional, List\n",
    "\n",
    "    def _wrap_text(s: str, width: Optional[int]) -> str:\n",
    "        return textwrap.fill(str(s), width=width) if width and width > 0 else str(s)\n",
    "\n",
    "    # --- Validation ---\n",
    "    required = {\"Answer\", \"Year\", \"Category\", \"Count\"}\n",
    "    missing = required - set(trend_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"trend_df missing required columns: {sorted(missing)}\")\n",
    "    if not categories:\n",
    "        raise ValueError(\"`categories` must be a non-empty list.\")\n",
    "\n",
    "    # Styling\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"notebook\", font_scale=font_scale)\n",
    "\n",
    "    # --- 1) Answer selection ---\n",
    "    answers = select_answers(trend_df, top_n=top_n, ensure_top_n_per_year=ensure_top_n_per_year)\n",
    "    if strict_top_n and len(answers) > top_n:\n",
    "        global_totals = (\n",
    "            trend_df.groupby(\"Answer\", as_index=False)\n",
    "                    .agg(total=(\"Count\", \"sum\"))\n",
    "                    .sort_values(by=[\"total\", \"Answer\"], ascending=[False, True])\n",
    "        )\n",
    "        selected: List[str] = []\n",
    "        allowed = set(map(str, answers))\n",
    "        for a in global_totals[\"Answer\"].astype(str):\n",
    "            if a in allowed:\n",
    "                selected.append(a)\n",
    "                if len(selected) >= top_n:\n",
    "                    break\n",
    "        answers = selected\n",
    "\n",
    "    if not answers:\n",
    "        raise ValueError(\"No answers selected—check data or selection parameters.\")\n",
    "\n",
    "    df_all = trend_df[trend_df[\"Answer\"].astype(str).isin(answers)].copy()\n",
    "    if df_all.empty:\n",
    "        raise ValueError(\"Filtered data is empty after selecting answers; nothing to plot.\")\n",
    "\n",
    "    # --- 2) Heatmaps preparation ---\n",
    "    df_heat, ylab_heat = add_relative(df_all, base_counts, mode=relative_heatmap)\n",
    "    if df_heat.empty:\n",
    "        raise ValueError(\"No heatmap data after applying heatmap relative mode.\")\n",
    "\n",
    "    years = sorted(df_heat[\"Year\"].dropna().unique().tolist())\n",
    "    if not years:\n",
    "        raise ValueError(\"No 'Year' values available for heatmaps.\")\n",
    "\n",
    "    ans_order = (\n",
    "        df_heat.groupby(\"Answer\", as_index=False)\n",
    "               .agg(total=(\"Value\", \"sum\"))\n",
    "               .sort_values(by=[\"total\", \"Answer\"], ascending=[False, True])[\"Answer\"]\n",
    "               .astype(str).tolist()\n",
    "    )\n",
    "    fmt_heat = \".1f\" if relative_heatmap else \".0f\"\n",
    "\n",
    "    # --- 3) Bars preparation ---\n",
    "    df_bars, ylab_bars = add_relative(df_all, base_counts, mode=relative_bars)\n",
    "    if df_bars.empty:\n",
    "        raise ValueError(\"No bar data after applying bar relative mode.\")\n",
    "\n",
    "    pivoted = (\n",
    "        df_bars.pivot_table(\n",
    "            index=[\"Answer\", \"Year\"],\n",
    "            columns=\"Category\",\n",
    "            values=\"Value\",\n",
    "            fill_value=0.0,\n",
    "            aggfunc=\"sum\",\n",
    "        ).reset_index()\n",
    "    )\n",
    "    for cat in categories:\n",
    "        if cat not in pivoted.columns:\n",
    "            pivoted[cat] = 0.0\n",
    "\n",
    "    totals_raw = (\n",
    "        df_all[df_all[\"Category\"].isin(categories)]\n",
    "        .groupby([\"Answer\", \"Year\"], as_index=False)\n",
    "        .agg(total_count=(\"Count\", \"sum\"))\n",
    "    )\n",
    "\n",
    "    include_total_bar = (relative_bars is None)\n",
    "\n",
    "    # Common y-limit across answer subplots\n",
    "    if relative_bars == \"answer\":\n",
    "        ymax_bars = 100.0\n",
    "    else:\n",
    "        ymax_bars = float(pivoted[categories].sum(axis=1).max())\n",
    "        if ymax_bars <= 0:\n",
    "            ymax_bars = 1.0\n",
    "\n",
    "    # --- 4) Dynamic sizing ---\n",
    "    n_cats = max(1, len(categories))\n",
    "    n_answer_bars = max(1, len(answers))\n",
    "    n_bars_total = n_answer_bars + (1 if include_total_bar else 0)\n",
    "\n",
    "    n_bar_cols = max(1, bar_cols)\n",
    "    n_bar_rows = math.ceil(n_bars_total / n_bar_cols)\n",
    "\n",
    "    heatmap_width = max(12.0, n_cats * heatmap_width_per_cat)\n",
    "    bars_width = n_bar_cols * bar_width_per_col\n",
    "    fig_width = max(heatmap_width, bars_width)\n",
    "    fig_height = top_row_height + n_bar_rows * bar_row_height\n",
    "\n",
    "    # --- 5) GridSpec scaffolding ---\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    outer_gs = fig.add_gridspec(\n",
    "        nrows=2, ncols=1,\n",
    "        height_ratios=[top_row_height, n_bar_rows * bar_row_height],\n",
    "        hspace=gridspec_hspace,\n",
    "    )\n",
    "    gs_heatmaps = outer_gs[0].subgridspec(1, n_cats, wspace=heatmap_wspace)\n",
    "    gs_bars = outer_gs[1].subgridspec(n_bar_rows, n_bar_cols, wspace=bar_wspace, hspace=bar_hspace)\n",
    "\n",
    "    # --- 5a) Heatmaps ---\n",
    "    heat_axes = []\n",
    "    vmin, vmax = float(df_heat[\"Value\"].min()), float(df_heat[\"Value\"].max())\n",
    "\n",
    "    for i, cat in enumerate(categories):\n",
    "        ax = fig.add_subplot(gs_heatmaps[0, i])\n",
    "        heat_axes.append(ax)\n",
    "\n",
    "        sub = df_heat[df_heat[\"Category\"] == cat]\n",
    "        mat = (\n",
    "            sub.pivot_table(index=\"Answer\", columns=\"Year\", values=\"Value\", fill_value=0.0)\n",
    "               .reindex(index=ans_order, columns=years)\n",
    "               .fillna(0.0)\n",
    "        )\n",
    "\n",
    "        sns.heatmap(\n",
    "            mat,\n",
    "            fmt=fmt_heat,\n",
    "            cmap=cmap,\n",
    "            ax=ax,\n",
    "            cbar=False,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=annotate,\n",
    "            annot_kws={\"fontsize\": int(annot_fontsize)} if annotate else None,\n",
    "        )\n",
    "        ax.set_title(str(cat), fontsize=12, pad=8)\n",
    "        ax.set_xlabel(\"Year\", fontsize=10, labelpad=xlabel_pad)\n",
    "        if show_all_heatmap_y or i == 0:\n",
    "            ax.tick_params(axis=\"y\", labelsize=heatmap_tick_fontsize)\n",
    "        else:\n",
    "            ax.tick_params(axis=\"y\", labelleft=False)\n",
    "        ax.tick_params(axis=\"x\", labelsize=10)\n",
    "\n",
    "    if heat_axes:\n",
    "        fig.colorbar(\n",
    "            heat_axes[-1].collections[0],\n",
    "            ax=heat_axes,\n",
    "            orientation=\"vertical\",\n",
    "            fraction=cbar_fraction,\n",
    "            pad=cbar_pad,\n",
    "        )\n",
    "\n",
    "    # --- 5b) Bars ---\n",
    "    palette = dict(zip(categories, sns.color_palette(\"tab20\", n_colors=len(categories))))\n",
    "    global_handles = [Patch(facecolor=palette[c], edgecolor=\"none\") for c in categories]\n",
    "\n",
    "    first_bar_ax = None  # for sharey across answer subplots (Totals excluded)\n",
    "\n",
    "    # Helper to set y-label & y-ticks visibility consistently\n",
    "    def _apply_yaxis_visibility(ax, is_first_col: bool, label_text: str):\n",
    "        \"\"\"\n",
    "        Controls both y-axis label and y-ticks (marks + labels) based on show_bar_ylabel.\n",
    "        - If show_bar_ylabel=True: show everywhere.\n",
    "        - If show_bar_ylabel=False: show only in first column; hide elsewhere.\n",
    "        \"\"\"\n",
    "        if show_bar_ylabel or is_first_col:\n",
    "            ax.set_ylabel(label_text, fontsize=9, labelpad=ylabel_pad)\n",
    "            ax.tick_params(axis=\"y\", labelleft=True, left=True)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "            ax.tick_params(axis=\"y\", labelleft=False, left=False)\n",
    "\n",
    "    # Totals bar as first subplot (only when absolute bars)\n",
    "    if include_total_bar:\n",
    "        total_slot = 0\n",
    "        row_idx = total_slot // n_bar_cols\n",
    "        col_idx = total_slot % n_bar_cols\n",
    "        ax_total = fig.add_subplot(gs_bars[row_idx, col_idx])  # not sharey with answers\n",
    "\n",
    "        totals_per_year = (\n",
    "            df_all[df_all[\"Category\"].isin(categories)]\n",
    "            .groupby(\"Year\", as_index=False)\n",
    "            .agg(total=(\"Count\", \"sum\"))\n",
    "            .sort_values(\"Year\")\n",
    "        )\n",
    "        xs = totals_per_year[\"Year\"].tolist()\n",
    "        ys = totals_per_year[\"total\"].astype(float).tolist()\n",
    "\n",
    "        ax_total.bar(xs, ys, color=\"#6E6E6E\", alpha=0.95, linewidth=0)\n",
    "        totals_ymax = max(1.0, float(max(ys) if ys else 1.0))\n",
    "        ax_total.set_ylim(0, totals_ymax * 1.08)\n",
    "\n",
    "        if annotate_bar_totals and len(xs) > 0:\n",
    "            y_offset = 0.02 * totals_ymax\n",
    "            for x, y in zip(xs, ys):\n",
    "                ax_total.text(\n",
    "                    x, y + y_offset,\n",
    "                    f\"N={y:,.0f}\",\n",
    "                    ha=\"center\", va=\"bottom\",\n",
    "                    fontsize=max(7, bar_tick_fontsize - 1),\n",
    "                    color=\"#333333\",\n",
    "                )\n",
    "\n",
    "        ax_total.set_title(\"Total (all selected answers)\", fontsize=bar_title_fontsize, pad=6)\n",
    "        ax_total.set_xticks(xs)\n",
    "        if rotate_bar_xticks:\n",
    "            ax_total.set_xticklabels(xs, rotation=rotate_bar_xticks, ha=\"right\")\n",
    "        ax_total.tick_params(axis=\"x\", labelsize=bar_tick_fontsize, pad=2)\n",
    "        ax_total.yaxis.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "        ax_total.set_axisbelow(True)\n",
    "        ax_total.margins(x=0.05)\n",
    "\n",
    "        # Apply y-axis visibility logic (Totals is in col 0)\n",
    "        _apply_yaxis_visibility(ax_total, is_first_col=True, label_text=\"Total Count\")\n",
    "\n",
    "    # Answer subplots (start after Totals if present)\n",
    "    for j, ans in enumerate(answers):\n",
    "        slot = j + (1 if include_total_bar else 0)\n",
    "        row_idx = slot // n_bar_cols\n",
    "        col_idx = slot % n_bar_cols\n",
    "\n",
    "        ax = fig.add_subplot(\n",
    "            gs_bars[row_idx, col_idx],\n",
    "            sharey=first_bar_ax if (share_y and first_bar_ax is not None) else None\n",
    "        )\n",
    "        if first_bar_ax is None:\n",
    "            first_bar_ax = ax  # first **answer** axis sets the shared scale\n",
    "\n",
    "        sub = pivoted[pivoted[\"Answer\"].astype(str) == str(ans)].sort_values(\"Year\")\n",
    "        xs = sub[\"Year\"].tolist()\n",
    "\n",
    "        bottoms = None\n",
    "        for cat in categories:\n",
    "            vals = sub[cat].to_numpy(dtype=float, copy=False)\n",
    "            ax.bar(xs, vals, bottom=bottoms, color=palette[cat], linewidth=0, alpha=0.95)\n",
    "            bottoms = vals if bottoms is None else (bottoms + vals)\n",
    "\n",
    "        ax.set_ylim(0, ymax_bars)\n",
    "\n",
    "        # Annotations for raw totals above the stacks\n",
    "        if annotate_bar_totals and len(xs) > 0:\n",
    "            stack_tops = sub[categories].sum(axis=1).to_numpy(dtype=float, copy=False)\n",
    "            sub_totals = (\n",
    "                totals_raw[totals_raw[\"Answer\"].astype(str) == str(ans)]\n",
    "                .set_index(\"Year\")\n",
    "                .reindex(xs)[\"total_count\"]\n",
    "                .fillna(0.0)\n",
    "                .to_numpy(dtype=float)\n",
    "            )\n",
    "            y_offset = 0.02 * ymax_bars\n",
    "            for x, top, raw_n in zip(xs, stack_tops, sub_totals):\n",
    "                ax.text(\n",
    "                    x, top + y_offset,\n",
    "                    f\"N={raw_n:,.0f}\",\n",
    "                    ha=\"center\", va=\"bottom\",\n",
    "                    fontsize=max(7, bar_tick_fontsize - 1),\n",
    "                    color=\"#333333\",\n",
    "                )\n",
    "\n",
    "        # Per-subplot legend (optional, with wrapping)\n",
    "        if legend_per_subplot:\n",
    "            handles = [Patch(facecolor=palette[c], edgecolor=\"none\") for c in categories]\n",
    "            labels_wrapped = [_wrap_text(c, legend_wrap_chars) for c in categories]\n",
    "            ax.legend(\n",
    "                handles=handles,\n",
    "                labels=labels_wrapped,\n",
    "                loc=\"upper right\",\n",
    "                frameon=False,\n",
    "                fontsize=max(8, bar_tick_fontsize - 1),\n",
    "                handlelength=1.2,\n",
    "                handletextpad=0.6,\n",
    "                borderaxespad=0.4,\n",
    "            )\n",
    "\n",
    "        # Readability\n",
    "        ax.set_title(str(ans), fontsize=bar_title_fontsize, pad=6)\n",
    "        ax.set_xticks(xs)\n",
    "        if rotate_bar_xticks:\n",
    "            ax.set_xticklabels(xs, rotation=rotate_bar_xticks, ha=\"right\")\n",
    "        ax.tick_params(axis=\"x\", labelsize=bar_tick_fontsize, pad=2)\n",
    "        ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.margins(x=0.05)\n",
    "\n",
    "        # Apply y-axis visibility logic for each answer subplot\n",
    "        _apply_yaxis_visibility(ax, is_first_col=(col_idx == 0), label_text=ylab_bars)\n",
    "\n",
    "    # --- Global legend (if not per-subplot) with wrapped labels ---\n",
    "    if not legend_per_subplot and categories:\n",
    "        wrapped_labels = [_wrap_text(c, legend_wrap_chars) for c in categories]\n",
    "        title_width = legend_title_wrap_chars if legend_title_wrap_chars is not None else legend_wrap_chars\n",
    "        wrapped_title = _wrap_text(legend_title, title_width)\n",
    "        fig.legend(\n",
    "            [Patch(facecolor=palette[c], edgecolor=\"none\") for c in categories],\n",
    "            wrapped_labels,\n",
    "            title=wrapped_title,\n",
    "            bbox_to_anchor=(0.99, 0.5),\n",
    "            loc=\"center left\",\n",
    "            frameon=False,\n",
    "            handlelength=1.2,\n",
    "            handletextpad=0.6,\n",
    "            borderaxespad=0.6,\n",
    "        )\n",
    "\n",
    "    # --- Title & layout ---\n",
    "    fig.suptitle(\n",
    "        f\"Dashboard for Top {top_n} Answers\\n\"\n",
    "        f\"Heatmaps ({ylab_heat}) + Stacked Bars ({ylab_bars})\"\n",
    "        f\"{' + Totals' if include_total_bar else ''}\\n\",\n",
    "        fontsize=15,\n",
    "        y=0.995,\n",
    "    )\n",
    "\n",
    "    right_limit = max(0.0, 1.0 - (legend_right_pad if not legend_per_subplot else 0.02))\n",
    "    fig.subplots_adjust(left=0.05, right=right_limit, top=0.9, bottom=0.05)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyzing and plotting the data for question 2: How do StackOverflow users perceive AI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# AISelect Questions - preparations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1) Prepare inputs\n",
    "survey_dict = {\n",
    "    \"2023\": survey_dfs[\"survey_2023\"],\n",
    "    \"2024\": survey_dfs[\"survey_2024\"],\n",
    "    \"2025\": survey_dfs[\"survey_2025\"],\n",
    "}\n",
    "\n",
    "columns_to_analyze_AISelect = [\n",
    "    \"AISelect\"\n",
    "]\n",
    "\n",
    "# Optional: map raw column names to category labels.\n",
    "# If omitted, the raw column names will appear in the plots.\n",
    "category_map_AISelect = {\n",
    "    \"AISelect\": \"Do you currently use AI tools in your development process?\"\n",
    "}\n",
    "\n",
    "# 2) Combine surveys\n",
    "long_df = prepare_long_format(survey_dict, columns_to_analyze_AISelect)\n",
    "\n",
    "# 3) Build Answer × Year × Category counts (generic)\n",
    "trend_df_AISelect, base_counts_AISelect, categories_AISelect = compute_trend_generic(\n",
    "    long_df,\n",
    "    columns_to_analyze_AISelect,\n",
    "    category_map=category_map_AISelect,  # or None to use column names as categories\n",
    "    answers=None,               # keep all answers; you could pass a list to restrict\n",
    "    deduplicate_within_row=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_dashboard(\n",
    "    trend_df=trend_df_AISelect,                 # long DF: ['Answer','Year','Category','Count']\n",
    "    base_counts=base_counts_AISelect,           # per-year totals for normalization (relative='year')\n",
    "    categories=categories_AISelect,             # list of category labels (3 in your case)\n",
    "\n",
    "    top_n=7,                           # how many answers to display\n",
    "    ensure_top_n_per_year=True,        # union of per-year top-N answers\n",
    "    strict_top_n=True,                 # cap final selection back to exactly top_n\n",
    "\n",
    "    relative_heatmap=\"year\",           # heatmap values as % of respondents per year\n",
    "    relative_bars=None,                # 100% stacks (category share)\n",
    "    share_y=True,                      # all at 0–100\n",
    "    annotate_bar_totals=False,         # write raw N above each bar\n",
    "    show_bar_ylabel = True,            # show ylabel for each bar plot\n",
    "\n",
    "    bar_cols=4,                        # 4 bar subplots per row (8 → 2 rows)\n",
    "\n",
    "    top_row_height=5.0,                # height (in) of the heatmaps row\n",
    "    bar_row_height=5.0,                # height (in) per row of bar charts\n",
    "    heatmap_width_per_cat=5.5,         # width (in) per heatmap column\n",
    "    bar_width_per_col=4.0,             # width (in) per bar subplot\n",
    "\n",
    "    font_scale=1.0,                    # global font scaling\n",
    "    rotate_bar_xticks=30,              # rotate bar x-axis tick labels (deg)\n",
    "\n",
    "    # Spacing tuned for your layout\n",
    "    heatmap_wspace=0.2,                # horizontal gap between heatmaps\n",
    "    bar_wspace=0.5,                    # horizontal gap between bar subplots\n",
    "    bar_hspace=0.5,                    # vertical gap between bar rows\n",
    "    gridspec_hspace=0.3,               # vertical gap between heatmaps block and bars block\n",
    "    legend_right_pad=0.15,             # fraction of figure width reserved for right-side legend\n",
    "\n",
    "    # Annotation and ticks\n",
    "    annot_fontsize=9,                  # font size for heatmap cell annotations\n",
    "    show_all_heatmap_y=False           # show y-ticks only on leftmost heatmap (saves space)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare better over the years, we will introduce a general \"yes\" answer for the survey of 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_MERGE_MAP = {\n",
    "    \"Yes, I use AI tools daily\": \"Yes\",\n",
    "    \"Yes, I use AI tools weekly\": \"Yes\",\n",
    "    \"Yes, I use AI tools monthly or infrequently\": \"Yes\"\n",
    "    # add more mappings as needed…\n",
    "}\n",
    "\n",
    "trend_df_merged = (\n",
    "    trend_df_AISelect\n",
    "    .assign(Answer=lambda d: d[\"Answer\"].astype(str).replace(YES_MERGE_MAP))\n",
    "    .groupby([\"Answer\", \"Year\", \"Category\"], as_index=False, sort=False)[\"Count\"]\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_dashboard(\n",
    "    trend_df=trend_df_merged,                   # long DF: ['Answer','Year','Category','Count']\n",
    "    base_counts=base_counts_AISelect,           # per-year totals for normalization (relative='year')\n",
    "    categories=categories_AISelect,             # list of category labels (3 in your case)\n",
    "\n",
    "    top_n=4,                           # how many answers to display\n",
    "    ensure_top_n_per_year=True,        # union of per-year top-N answers\n",
    "    strict_top_n=True,                 # cap final selection back to exactly top_n\n",
    "\n",
    "    relative_heatmap=\"year\",           # heatmap values as % of respondents per year\n",
    "    relative_bars=None,                # 100% stacks (category share)\n",
    "    share_y=True,                      # all at 0–100\n",
    "    annotate_bar_totals=False,         # write raw N above each bar\n",
    "    show_bar_ylabel = True,            # show ylabel for each bar plot\n",
    "\n",
    "    bar_cols=3,                        # 4 bar subplots per row (8 → 2 rows)\n",
    "\n",
    "    top_row_height=5.0,                # height (in) of the heatmaps row\n",
    "    bar_row_height=5.0,                # height (in) per row of bar charts\n",
    "    heatmap_width_per_cat=4.5,         # width (in) per heatmap column\n",
    "    bar_width_per_col=4.0,             # width (in) per bar subplot\n",
    "\n",
    "    font_scale=1.0,                    # global font scaling\n",
    "    rotate_bar_xticks=30,              # rotate bar x-axis tick labels (deg)\n",
    "\n",
    "    # Spacing tuned for your layout\n",
    "    heatmap_wspace=0.2,                # horizontal gap between heatmaps\n",
    "    bar_wspace=0.5,                    # horizontal gap between bar subplots\n",
    "    bar_hspace=0.5,                    # vertical gap between bar rows\n",
    "    gridspec_hspace=0.3,               # vertical gap between heatmaps block and bars block\n",
    "    legend_right_pad=0.15,             # fraction of figure width reserved for right-side legend\n",
    "\n",
    "    # Annotation and ticks\n",
    "    annot_fontsize=9,                  # font size for heatmap cell annotations\n",
    "    show_all_heatmap_y=False           # show y-ticks only on leftmost heatmap (saves space)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of this question and the above plots it becomes evident that the general usage of AI has increased from 2023 over 2024 to 2025. In 2023, less than half the survey participants were using AI according to the above heatmap. This shifted in 2024 where now 57.6% of the participants are using AI. Even though a third of the participants did not answer this questions, among the other two thirds the AI-users make up a clear majority.\n",
    "\n",
    "When taking into account the more detailed \"yes\"-answers in the survey of 2025 we can also see that if the participants use AI, they mostly do so on a *daily basis*.\n",
    "\n",
    "We will investigate the distribution of \"no answer\" later in order to see if this is a general effect of the study of 2025 or if this is specific to a certain question or the AI-topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# AISent Questions - preparations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1) Prepare inputs\n",
    "columns_to_analyze_AISent = [\n",
    "    \"AISent\"\n",
    "]\n",
    "\n",
    "# Optional: map raw column names to category labels.\n",
    "# If omitted, the raw column names will appear in the plots.\n",
    "category_map_AISent = {\n",
    "    \"AISent\": \"How favorable is your stance on using AI tools as part of your development workflow?\"\n",
    "}\n",
    "\n",
    "# 2) Combine surveys\n",
    "long_df = prepare_long_format(survey_dict, columns_to_analyze_AISent)\n",
    "\n",
    "# 3) Build Answer × Year × Category counts (generic)\n",
    "trend_df_AISent, base_counts_AISent, categories_AISent= compute_trend_generic(\n",
    "    long_df,\n",
    "    columns_to_analyze_AISent,\n",
    "    category_map=category_map_AISent,  # or None to use column names as categories\n",
    "    answers=None,               # keep all answers; you could pass a list to restrict\n",
    "    deduplicate_within_row=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_dashboard(\n",
    "    trend_df=trend_df_AISent,                 # long DF: ['Answer','Year','Category','Count']\n",
    "    base_counts=base_counts_AISent,           # per-year totals for normalization (relative='year')\n",
    "    categories=categories_AISent,             # list of category labels (3 in your case)\n",
    "\n",
    "    top_n=7,                           # how many answers to display\n",
    "    ensure_top_n_per_year=True,        # union of per-year top-N answers\n",
    "    strict_top_n=True,                 # cap final selection back to exactly top_n\n",
    "\n",
    "    relative_heatmap=\"year\",           # heatmap values as % of respondents per year\n",
    "    relative_bars=None,                # 100% stacks (category share)\n",
    "    share_y=True,                      # all at 0–100\n",
    "    annotate_bar_totals=False,          # write raw N above each bar\n",
    "    show_bar_ylabel = True,            # show ylabel for each bar plot\n",
    "\n",
    "    bar_cols=4,                        # 4 bar subplots per row (8 → 2 rows)\n",
    "\n",
    "    top_row_height=5.0,                # height (in) of the heatmaps row\n",
    "    bar_row_height=5.0,                # height (in) per row of bar charts\n",
    "    heatmap_width_per_cat=4.5,         # width (in) per heatmap column\n",
    "    bar_width_per_col=4.0,             # width (in) per bar subplot\n",
    "\n",
    "    font_scale=1.0,                    # global font scaling\n",
    "    rotate_bar_xticks=30,              # rotate bar x-axis tick labels (deg)\n",
    "\n",
    "    # Spacing tuned for your layout\n",
    "    heatmap_wspace=0.2,                # horizontal gap between heatmaps\n",
    "    bar_wspace=0.5,                    # horizontal gap between bar subplots\n",
    "    bar_hspace=0.5,                    # vertical gap between bar rows\n",
    "    gridspec_hspace=0.3,               # vertical gap between heatmaps block and bars block\n",
    "    legend_right_pad=0.15,             # fraction of figure width reserved for right-side legend\n",
    "\n",
    "    # Annotation and ticks\n",
    "    annot_fontsize=9,                  # font size for heatmap cell annotations\n",
    "    show_all_heatmap_y=False           # show y-ticks only on leftmost heatmap (saves space)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the question \"AISelect\" investigated before, the proportion of the survey participants not answering this question regarding the stance to use AI in their workflow is roughly 30% over all the surveys of 2023, 2024, and 2024. However, among the participants that answered this question, AI is perceived largely positive (\"favorable\" or \"very favorable\") with a slight downward trend towards 2025. In fact, in this year the negative answers (\"unfavorable\" or \"very unfavorable\") experienced an increase which was quite noticable for the \"very unfavorable\" option. \n",
    "\n",
    "Summing up we can say that AI is perceived positively in the context of using it in the development workflow, however, with a slight downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# AIAcc Questions - preparations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1) Prepare inputs\n",
    "columns_to_analyze_AIAcc= [\n",
    "    \"AIAcc\"\n",
    "]\n",
    "\n",
    "# Optional: map raw column names to category labels.\n",
    "# If omitted, the raw column names will appear in the plots.\n",
    "category_map_AIAcc = {\n",
    "    \"AIAcc\": \"How much do you trust the accuracy of the output from AI tools as part of your development workflow?\"\n",
    "}\n",
    "\n",
    "# 2) Combine surveys\n",
    "long_df = prepare_long_format(survey_dict, columns_to_analyze_AIAcc)\n",
    "\n",
    "# 3) Build Answer × Year × Category counts (generic)\n",
    "trend_df_AIAcc, base_counts_AIAcc, categories_AIAcc= compute_trend_generic(\n",
    "    long_df,\n",
    "    columns_to_analyze_AIAcc,\n",
    "    category_map=category_map_AIAcc,  # or None to use column names as categories\n",
    "    answers=None,               # keep all answers; you could pass a list to restrict\n",
    "    deduplicate_within_row=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_dashboard(\n",
    "    trend_df=trend_df_AIAcc,                   # long DF: ['Answer','Year','Category','Count']\n",
    "    base_counts=base_counts_AIAcc,           # per-year totals for normalization (relative='year')\n",
    "    categories=categories_AIAcc,             # list of category labels (3 in your case)\n",
    "\n",
    "    top_n=6,                           # how many answers to display\n",
    "    ensure_top_n_per_year=True,        # union of per-year top-N answers\n",
    "    strict_top_n=True,                 # cap final selection back to exactly top_n\n",
    "\n",
    "    relative_heatmap=\"year\",           # heatmap values as % of respondents per year\n",
    "    relative_bars=None,                # 100% stacks (category share)\n",
    "    share_y=True,                      # all at 0–100\n",
    "    annotate_bar_totals=False,          # write raw N above each bar\n",
    "    show_bar_ylabel = True,            # show ylabel for each bar plot\n",
    "\n",
    "    bar_cols=4,                        # 4 bar subplots per row (8 → 2 rows)\n",
    "\n",
    "    top_row_height=5.0,                # height (in) of the heatmaps row\n",
    "    bar_row_height=5.0,                # height (in) per row of bar charts\n",
    "    heatmap_width_per_cat=4.5,         # width (in) per heatmap column\n",
    "    bar_width_per_col=4.0,             # width (in) per bar subplot\n",
    "\n",
    "    font_scale=1.0,                    # global font scaling\n",
    "    rotate_bar_xticks=30,              # rotate bar x-axis tick labels (deg)\n",
    "\n",
    "    # Spacing tuned for your layout\n",
    "    heatmap_wspace=0.2,                # horizontal gap between heatmaps\n",
    "    bar_wspace=0.5,                   # horizontal gap between bar subplots\n",
    "    bar_hspace=0.5,                    # vertical gap between bar rows\n",
    "    gridspec_hspace=0.3,               # vertical gap between heatmaps block and bars block\n",
    "    legend_right_pad=0.15,             # fraction of figure width reserved for right-side legend\n",
    "\n",
    "    # Annotation and ticks\n",
    "    annot_fontsize=9,                  # font size for heatmap cell annotations\n",
    "    show_all_heatmap_y=False           # show y-ticks only on leftmost heatmap (saves space)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the question about the trustlevel that the StackOverflow users have towards the output of AI, we can note that the proportion of \"no answer\" is even higher than before. Almost half of the participants of the 2024 survey did not answer this question. Among the rest, the trust in the output of AI tools is slightly positive with a clear trend towards distrust, which becomes evident in the survey of 2025 where 13.3% of the participants stated they \"highly distrust\" the results. \n",
    "\n",
    "#### Summary for question 2: How do StackOverflow users perceive AI?\n",
    "This downward trend in the perception of AI as seen by comparing the, its usage and results can have several reasons. On the one hand, the number of available AI tools is increasing extremely fast and it is becoming more and more easy to use these tools also in a professional work environment. Therefore, the tools are no longer seen as magically solving toy problems with AI as seen in media - but they are actually used for real problems, where some AI tools still have their limits. In addition, with the wider spread of AI tools the users and the general population itself are sensitized or educated about a responsible and careful usage of the tools. Therefore, it makes sense that the optimism regarding AI tools seems to slowly decline to a more realistic perception of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Analyzing and plotting the data for question 3: for which purpose is AI used by the StackOverflow users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# AITool Questions - preparations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "columns_to_analyze_AITool = [\n",
    "    \"AIToolCurrently Using\",\n",
    "    \"AIToolInterested in Using\",\n",
    "    \"AIToolNot interested in Using\",\n",
    "]\n",
    "\n",
    "# Optional: map raw column names to category labels.\n",
    "# If omitted, the raw column names will appear in the plots.\n",
    "category_map_AITool = {\n",
    "    \"AIToolCurrently Using\": \"Currently Using\",\n",
    "    \"AIToolInterested in Using\": \"Interested in Using\",\n",
    "    \"AIToolNot interested in Using\": \"Not Interested\",\n",
    "}\n",
    "\n",
    "# 2) Combine surveys\n",
    "long_df = prepare_long_format(survey_dict, columns_to_analyze_AITool)\n",
    "\n",
    "# 3) Build Answer × Year × Category counts (generic)\n",
    "trend_df_AITool, base_counts_AITool, categories_AITool = compute_trend_generic(\n",
    "    long_df,\n",
    "    columns_to_analyze_AITool,\n",
    "    category_map=category_map_AITool,  # or None to use column names as categories\n",
    "    answers=None,               # keep all answers; you could pass a list to restrict\n",
    "    deduplicate_within_row=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# AITool Questions - plotting the results\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "plot_combined_dashboard(\n",
    "    trend_df=trend_df_AITool,                 # long DF: ['Answer','Year','Category','Count']\n",
    "    base_counts=base_counts_AITool,           # per-year totals for normalization (relative='year')\n",
    "    categories=categories_AITool,             # list of category labels (3 in your case)\n",
    "\n",
    "    top_n=15,                           # how many answers to display\n",
    "    ensure_top_n_per_year=True,        # union of per-year top-N answers\n",
    "    strict_top_n=True,                 # cap final selection back to exactly top_n\n",
    "\n",
    "    relative_heatmap=\"year\",           # heatmap values as % of respondents per year\n",
    "    relative_bars=\"answer\",            # 100% stacks (category share)\n",
    "    share_y=True,                      # all at 0–100\n",
    "    annotate_bar_totals=False,         # write raw N above each bar\n",
    "    show_bar_ylabel = False,            # show ylabel for each bar plot\n",
    "\n",
    "    bar_cols=5,                        # 4 bar subplots per row (8 → 2 rows)\n",
    "\n",
    "    top_row_height=5.0,                # height (in) of the heatmaps row\n",
    "    bar_row_height=5.0,                # height (in) per row of bar charts\n",
    "    heatmap_width_per_cat=4.5,         # width (in) per heatmap column\n",
    "    bar_width_per_col=4.0,             # width (in) per bar subplot\n",
    "\n",
    "    font_scale=1.0,                    # global font scaling\n",
    "    rotate_bar_xticks=30,              # rotate bar x-axis tick labels (deg)\n",
    "\n",
    "    # Spacing tuned for your layout\n",
    "    heatmap_wspace=0.2,                # horizontal gap between heatmaps\n",
    "    bar_wspace=0.25,                   # horizontal gap between bar subplots\n",
    "    bar_hspace=0.5,                    # vertical gap between bar rows\n",
    "    gridspec_hspace=0.3,               # vertical gap between heatmaps block and bars block\n",
    "    legend_right_pad=0.15,             # fraction of figure width reserved for right-side legend\n",
    "\n",
    "    # Annotation and ticks\n",
    "    annot_fontsize=9,                  # font size for heatmap cell annotations\n",
    "    show_all_heatmap_y=False           # show y-ticks only on leftmost heatmap (saves space)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, the proportion of survey participants not answering was extremely high. Depending on the survey year and the type of question, up to 80% of all participants did not answer some of the questions. The reason for this can only be guessed like there were too many choices but there is no clear pattern as to why this question was avoided that much. \n",
    "\n",
    "However, from those who answered we can deduce a few takeaways regarding the purpose of AI usage:\n",
    "- Currently using\n",
    "    - \"Writing code\" is still the number one application of AI. Still, in 2025 the numbers are significantly lower than in 2024 and, of the people not currently using AI for that purpose, an increasing proportion is not even interested in using it in the future.\n",
    "    - Nevertheless, the field of coding is still a huge field of application for AI with several tools currently in use ranging from code documentation, over debugging to testing code. Ironically, we *are* analyzing a survey of StackOverflow users to the user base might be a little bit biased here :)\n",
    "    - Another hot topic currently being used is \"search for answers\", which is also a common use case among many other user groups as chatbots are taking over the popularity as very strong search engines providing precise answers and references to the search queries (depending of course on the precision of the query).\n",
    "- Not interested\n",
    "    - \"Comitting and reviewing code\" or \"Deployment and monitoring\" still seems to be preferred not to be done by AI which makes sense because we still need to question the output of the AI and, additionally, the trustlevel in AI ouput is slightly decreasing.\n",
    "    - \"Project planning\" seems to be quite unfavored among the StackOverflow users. This, however, might be biased by the user group as the majority might be more involved in coding instead of project planning. This could be evaluated more by making correlations to other survey questions regarding the general field of work of the survey participants.\n",
    "    - \"Predictive analytics\" is also increasingly uninteresting, probably also related to the decreasing trust in AI output.\n",
    "- Interested in using\n",
    "    - The fields of interest, on the other hand, are almost equally distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Proportion of \"no answer\"\n",
    "\n",
    "As the proportion of not answered questions is, in some cases, unexpectedly high, I want to analyze the proportion of the \"no answer\" values with respect to the other answers of the surveys in order to see if there is a trend for/against the AI-related questions or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "target_questions = (columns_to_analyze_AIAcc + columns_to_analyze_AISelect + columns_to_analyze_AISent + columns_to_analyze_AITool)\n",
    "\n",
    "# AI-related questions from df_matches\n",
    "ai_questions = df_matches.loc[df_matches[\"survey\"].isin([f\"survey_{y}\" for y in [\"2023\",\"2024\",\"2025\"]]), \"column_name\"].tolist()\n",
    "\n",
    "# Helper function to compute proportion of \"no answer\"\n",
    "def compute_no_answer_prop(df):\n",
    "    total = len(df)\n",
    "    no_ans = (df[\"answer\"].astype(str).str.lower() == \"no answer\").sum()\n",
    "    return no_ans / total if total > 0 else None\n",
    "\n",
    "rows = []\n",
    "\n",
    "for year, survey_df in survey_dict.items():\n",
    "    # Melt wide format to long format\n",
    "    df_long = survey_df.melt(var_name=\"question\", value_name=\"answer\")\n",
    "    \n",
    "    # Group 0: Target questions\n",
    "    target_df = df_long[df_long[\"question\"].isin(target_questions)]\n",
    "    target_prop = compute_no_answer_prop(target_df)\n",
    "\n",
    "    # Group 1: All other questions\n",
    "    other_df = df_long[~df_long[\"question\"].isin(target_questions)]\n",
    "    other_prop = compute_no_answer_prop(other_df)\n",
    "\n",
    "    # Group 2: Other questions containing \"AI\"\n",
    "    other_ai_df = other_df[other_df[\"question\"].isin(ai_questions)]\n",
    "    other_ai_prop = compute_no_answer_prop(other_ai_df)\n",
    "\n",
    "    # Group 3: Other questions NOT containing \"AI\"\n",
    "    other_non_ai_df = other_df[~other_df[\"question\"].isin(ai_questions)]\n",
    "    other_non_ai_prop = compute_no_answer_prop(other_non_ai_df)\n",
    "\n",
    "    # Group 4: All questions\n",
    "    all_prop = compute_no_answer_prop(df_long)\n",
    "\n",
    "    rows.append({\n",
    "        \"Year\": year,\n",
    "        \"Target Questions\": target_prop,\n",
    "        \"All Other\": other_prop,\n",
    "        \"Other AI\": other_ai_prop,\n",
    "        \"Other Non-AI\": other_non_ai_prop,\n",
    "        \"All\": all_prop\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Year as index for plotting\n",
    "plot_df = comparison_df.set_index(\"Year\")\n",
    "plot_df_reset = plot_df.reset_index().melt(id_vars=\"Year\", var_name=\"Group\", value_name=\"Proportion\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=plot_df_reset, x=\"Year\", y=\"Proportion\", hue=\"Group\")\n",
    "plt.title(\"Comparison of 'No Answer' Proportions by Year\")\n",
    "plt.ylabel(\"Proportion of 'No Answer'\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the proportions of the \"no answer\" values to the other questions, we can observe the following:\n",
    "- In average, the questions considered in this analysis were answered more properly over time with the fewest \"no answer\" values in the latest survey of 2025.\n",
    "- Compared to \"all other\" or even \"all\" questions, in 2023 the survey participants answered the target questions less, in 2024 equally as much and in 2025 clearly more frequent.\n",
    "- \"Other AI\"-related questions seem to be answered relatively sparsely when compared to \"other non-AI\".\n",
    "- The general proportion of \"no answer\" for all questions is increasing over time with a percentage of over 50% in the latest survey of 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How did the AI‑related survey questions develop over time?\n",
    "\n",
    "- Sharp rise in AI focus: The number of AI‑related items grows from 3 (2018) and 1 (2019) to 9 (2023), 11 (2024), and 31 (2025). By 2025, AI questions account for ~20% of the full survey.\n",
    "- Overall survey size dip & rebound: Total question count drops around 2020/2021, then rises again in 2023–2025 alongside the AI share.\n",
    "\n",
    "**Implication**: The survey increasingly probes AI—both breadth (more topics like tools, trust, agents) and depth (richer sub‑items in 2025)\n",
    "\n",
    "### Question 2: How do users perceive AI, and has that changed?\n",
    "\n",
    "The analysis uses three recurring questions—\"AISelect\" (use), \"AISent\" (stance), and \"AIAcc\" (trust). A 2023 schema mix‑up between \"AIBen\" and \"AIAcc\" is detected and corrected to ensure comparability across 2023–2025.\n",
    "\n",
    "- \"AISelect\": Usage rises over time. In 2024, the share using AI is ~57.6% (noting a sizeable non‑response). In 2025, the split reveals that most “Yes” users report daily usage.\n",
    "- \"AISent\": Majority remains favorable/very favorable, but there’s a slight downward trend into 2025 with a noticeable uptick in “very unfavorable.”\n",
    "- \"AIAcc\": Still net positive (“somewhat trust” leads) yet trending toward more distrust; in 2025, ~13.3% report “highly distrust.” Non‑response on trust is high (e.g., ~half in 2024).\n",
    "\n",
    "**Implication**: Usage keeps climbing, but attitudes are normalizing—optimism cooling slightly as real‑world limitations and risks surface.\n",
    "\n",
    "### Question 3: What do people use AI for?\n",
    "\n",
    "- Top current use: “Writing code” remains the #1 application; usage in 2025 is lower than 2024, and among non‑users for coding a growing share is not interested in adopting it. “Search for answers” is another strong, current use.\n",
    "- Less favored: “Committing/reviewing code” and “Deployment/monitoring” tend to show higher not‑interested rates. “Project planning” is also relatively unfavored (possibly role bias in the SO audience). “Predictive analytics” interest appears to wane, aligning with the trust findings.\n",
    "- Data note: The 2025 matrix question changed and was re‑mapped to align with 2023/2024 categories before comparison. Also, non‑response for this block is very high (up to ~80% on some items), so interpret with caution.\n",
    "\n",
    "### Additional data quality observations\n",
    "- Schema evolution (2017–2025) complicates strict year‑over‑year comparisons; recent years are more comparable.\n",
    "- Non‑response (“No answer”): For the AI questions analyzed, non‑response declines by 2025 relative to other questions, but overall survey non‑response rises over time (exceeding 50% in 2025 across all questions). This can bias levels and trends.\n",
    "\n",
    "### **Bottom line**\n",
    "- AI’s prominence in the survey has surged since 2023.\n",
    "- Usage keeps growing (with many daily users by 2025), yet sentiment and trust show a softening—especially in 2025.\n",
    "- Coding and answer‑search dominate today’s use; review/monitoring/planning remain less AI‑driven among respondents.\n",
    "- Interpretation caveat: High and shifting non‑response rates and schema changes require caution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
